version: "3.8"

services:
  qwen3-4b:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: qwen3-4b-server
    platform: linux/amd64
    ports:
      - "8081:8080"
    volumes:
      - ./models:/models
    # Add `-ngl 99` after `-c 32768` when running with GPU layers enabled.
    command: >
      -m /models/Qwen3-4B-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 32768
      --temp 0.6
      --top-k 20
      --top-p 0.95
      --verbose-prompt
    # CPU-only configuration
    # environment:
    #   - CUDA_VISIBLE_DEVICES=0
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# To use this docker-compose:
# 1. Create a models directory: mkdir models
# 2. Download the model (choose one method):
#    Method 1 - Official Qwen repo:
#    wget -O models/Qwen3-4B-Q4_K_M.gguf https://huggingface.co/Qwen/Qwen3-4B-GGUF/resolve/main/Qwen3-4B-Q4_K_M.gguf
#    
#    Method 2 - Using huggingface-cli:
#    huggingface-cli download Qwen/Qwen3-4B-GGUF Qwen3-4B-Q4_K_M.gguf --local-dir models
#    
#    Method 3 - Optimized LLMware version:
#    huggingface-cli download llmware/qwen3-4b-instruct-gguf qwen3-4b-instruct-q4_k_m.gguf --local-dir models
#    (then rename to Qwen3-4B-Q4_K_M.gguf)
#
# 3. Run: docker-compose -f qwen3-4b-docker-compose.yml up -d
#
# API Usage:
# - Chat API: http://localhost:8081/v1/chat/completions
# - Completions API: http://localhost:8081/v1/completions
# - Health check: http://localhost:8081/health
